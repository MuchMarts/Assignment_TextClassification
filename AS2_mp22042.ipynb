{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6/l6bKuP7qVw1JYdZ+nud",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuchMarts/Assignment_TextClassification/blob/main/AS2_mp22042.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2 - **Text classification**\n",
        "\n",
        "**Author:** Mārtiņš Patjanko  \n",
        "**Student ID:** mp22042\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "cMXYtGt-4ggH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and evaluate a text classifier\n",
        "\n",
        "Step 1: Download GoEmotions datasets - train/test/dev  \n",
        "Step 1.5: Download ekman emoption mapping needed later"
      ],
      "metadata": {
        "id": "0iwM_GPY5Npb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR6tzVEL30JN",
        "outputId": "5308e041-42ea-49fb-c3a2-1e0ebb84f988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-12 06:19:07--  https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3519053 (3.4M) [text/plain]\n",
            "Saving to: ‘train.tsv’\n",
            "\n",
            "train.tsv           100%[===================>]   3.36M  9.98MB/s    in 0.3s    \n",
            "\n",
            "2025-06-12 06:19:08 (9.98 MB/s) - ‘train.tsv’ saved [3519053/3519053]\n",
            "\n",
            "--2025-06-12 06:19:09--  https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 436706 (426K) [text/plain]\n",
            "Saving to: ‘test.tsv’\n",
            "\n",
            "test.tsv            100%[===================>] 426.47K  2.23MB/s    in 0.2s    \n",
            "\n",
            "2025-06-12 06:19:09 (2.23 MB/s) - ‘test.tsv’ saved [436706/436706]\n",
            "\n",
            "--2025-06-12 06:19:09--  https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/dev.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 439059 (429K) [text/plain]\n",
            "Saving to: ‘dev.tsv’\n",
            "\n",
            "dev.tsv             100%[===================>] 428.77K  1.98MB/s    in 0.2s    \n",
            "\n",
            "2025-06-12 06:19:11 (1.98 MB/s) - ‘dev.tsv’ saved [439059/439059]\n",
            "\n",
            "--2025-06-12 06:19:11--  https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/ekman_mapping.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 396 [text/plain]\n",
            "Saving to: ‘ekman_mapping.json’\n",
            "\n",
            "ekman_mapping.json  100%[===================>]     396  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-12 06:19:11 (21.6 MB/s) - ‘ekman_mapping.json’ saved [396/396]\n",
            "\n",
            "--2025-06-12 06:19:11--  https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/emotions.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248 [text/plain]\n",
            "Saving to: ‘emotions.txt’\n",
            "\n",
            "emotions.txt        100%[===================>]     248  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-12 06:19:12 (793 KB/s) - ‘emotions.txt’ saved [248/248]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Clean up directory\n",
        "![ -f train.tsv ] && rm train.tsv\n",
        "![ -f dev.tsv ] && rm dev.tsv\n",
        "![ -f test.tsv ] && rm test.tsv\n",
        "![ -f train_filtered.tsv ] && rm train_filtered.tsv\n",
        "![ -f dev_filtered.tsv ] && rm dev_filtered.tsv\n",
        "![ -f test_filtered.tsv ] && rm test_filtered.tsv\n",
        "![ -f ekman_mapping.json ] && rm ekman_mapping.json\n",
        "![ -f emotions.txt ] && rm emotions.txt\n",
        "\n",
        "# Datasets\n",
        "!wget https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/train.tsv\n",
        "!wget https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/test.tsv\n",
        "!wget https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/dev.tsv\n",
        "\n",
        "# Ekman mapping\n",
        "!wget https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/ekman_mapping.json\n",
        "\n",
        "# Emotion list\n",
        "!wget https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/emotions.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Convert all emotions to base emotions in mapping and remove entires with multiple emotions"
      ],
      "metadata": {
        "id": "WjKWSnJl5czz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Get all emotions, to know which id coresponds to what label\n",
        "with open('emotions.txt') as emotions:\n",
        "  goemotions_labels = emotions.read().splitlines()\n",
        "\n",
        "#print(goemotions_labels)\n",
        "\n",
        "# Load in ekman mapping and create a label_to_ekam mapping, which effectivly is a reverse index\n",
        "with open('ekman_mapping.json') as mapping:\n",
        "  ekman_mapping = json.load(mapping)\n",
        "\n",
        "#print(ekman_mapping)\n",
        "\n",
        "label_to_ekman = {}\n",
        "for ekman, labels in ekman_mapping.items():\n",
        "    for lbl in labels:\n",
        "        label_to_ekman[lbl] = ekman\n",
        "\n",
        "# Add neutral as it is not present in ekman_mapping\n",
        "label_to_ekman['neutral'] = 'neutral'\n",
        "\n",
        "\n",
        "def map_ids_to_ekman(id_list):\n",
        "    ekman_emotions = set()\n",
        "    for idx in id_list:\n",
        "        label = goemotions_labels[int(idx)]\n",
        "        ekman = label_to_ekman.get(label)\n",
        "        if ekman:\n",
        "            ekman_emotions.add(ekman)\n",
        "    return list(ekman_emotions)\n",
        "\n",
        "datasets = ['train', 'test', 'dev']\n",
        "\n",
        "for data_set in datasets:\n",
        "  df = pd.read_csv(f'{data_set}.tsv', sep='\\t', header=None)\n",
        "  # Split emotion label ids into a list and store in coll 2\n",
        "  df[2] = df[1].apply(lambda x: x.split(','))\n",
        "\n",
        "  # Replace emotion label ids with corresponding labels\n",
        "  df[2] = df[2].apply(map_ids_to_ekman)\n",
        "\n",
        "  # Store only entries where there is a singular emotion\n",
        "  df = df[df[2].apply(lambda x: len(x) == 1)]\n",
        "  df[2] = df[2].apply(lambda x: x[0])\n",
        "\n",
        "  df.to_csv(f'{data_set}_filtered.tsv', sep='\\t', header=False, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Hjp3Vy37t9y",
        "outputId": "77f44b3f-5924-4023-cd19-687bbc1006d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-1969762395>:46: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[2] = df[2].apply(lambda x: x[0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK7NntkrRejP",
        "outputId": "672b8a59-6bf4-47a2-dfc4-36192575d3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "If6_BeycU9QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tokenize(doc):\n",
        "  tokens = []\n",
        "\n",
        "  for token in doc:\n",
        "    # Remove punctuaction and stop words\n",
        "    # could also remove token.is_stop\n",
        "    # Temp removed token.is_punct check\n",
        "    if token.is_space:\n",
        "      continue\n",
        "\n",
        "    # Add placeholders\n",
        "    if token.like_num:\n",
        "      tokens.append('<NUM>')\n",
        "    elif token.like_url:\n",
        "      tokens.append('<URL>')\n",
        "    elif token.like_email:\n",
        "      tokens.append('<EMAIL>')\n",
        "    else:\n",
        "      # Lower case and do lemmas\n",
        "      tokens.append(token.lemma_)\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "VKM_6GfSb38V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# All datasets are stored in memory and arent saved to file, because I think it is not nescessary to write it to file\n",
        "# This means that if you want to read the dataset data, you use tokenize_datasets, key is any value from list datasets (train, test, dev)\n",
        "tokenized_datasets = {}\n",
        "\n",
        "for data_set in datasets:\n",
        "  df = pd.read_csv(f'{data_set}_filtered.tsv', sep='\\t', header=None)\n",
        "  texts = df[0].tolist()\n",
        "\n",
        "  # Batching to speed it up\n",
        "  docs = list(tqdm(nlp.pipe(texts, batch_size=500), total=len(texts)))\n",
        "\n",
        "  # Adjusted tokenization, to lower all text. Making capitalization not matter.\n",
        "  df[\"tokens\"] = [clean_tokenize(doc) for doc in docs]\n",
        "\n",
        "  tokenized_datasets[data_set] = df.copy()\n",
        "  #print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIgOv-B5Rp5G",
        "outputId": "988983d5-bfc2-4b45-b52b-f3f99f5a35f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 39555/39555 [01:12<00:00, 544.06it/s]\n",
            "100%|██████████| 4968/4968 [00:07<00:00, 622.63it/s]\n",
            "100%|██████████| 4946/4946 [00:08<00:00, 612.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze tokenizer freq and pruning"
      ],
      "metadata": {
        "id": "TIAxclXyXafh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for NGrams\n",
        "\n",
        "def get_ngrams(tokens, n):\n",
        "    return zip(*[tokens[i:] for i in range(n)])"
      ],
      "metadata": {
        "id": "eSxJH88taVZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "for data_set in datasets:\n",
        "  dataset_tokens = tokenized_datasets[data_set]\n",
        "\n",
        "  unigrams = Counter()\n",
        "  for entry in dataset_tokens[\"tokens\"]:\n",
        "    for token in entry:\n",
        "      unigrams[token] += 1\n",
        "\n",
        "  print(f\"Dataset: {data_set}\")\n",
        "  print(f\"Top 20 unigrams\")\n",
        "  print(unigrams.most_common(20))\n",
        "\n",
        "  bigrams = Counter()\n",
        "  for entry in dataset_tokens['tokens']:\n",
        "    bigrams.update(get_ngrams(entry, 2))\n",
        "\n",
        "  print(f\"Top 20 bigrams\")\n",
        "  print(bigrams.most_common(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upQsL9F7XVfb",
        "outputId": "eec899a8-4b49-49a2-9261-f19b41978d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: train\n",
            "Top 20 unigrams\n",
            "[('.', 33321), ('be', 27433), ('I', 21262), ('the', 16233), (',', 12724), ('to', 11812), ('a', 11158), ('you', 10343), ('it', 9526), ('not', 9038), ('that', 8897), ('and', 8102), ('!', 8089), ('name', 7268), (']', 7191), ('[', 7179), ('do', 6948), ('<NUM>', 6767), ('of', 6429), ('have', 6125)]\n",
            "Top 20 bigrams\n",
            "[(('[', 'name'), 7010), (('name', ']'), 7010), (('do', 'not'), 3428), (('I', 'be'), 2900), (('it', 'be'), 2823), (('.', 'I'), 2387), (('be', 'a'), 2197), (('that', 'be'), 1735), (('be', 'not'), 1677), (('!', '!'), 1483), ((',', 'I'), 1471), (('you', 'be'), 1333), (('I', 'do'), 1331), (('this', 'be'), 1316), (('I', 'have'), 1262), (('in', 'the'), 1252), (('be', 'the'), 1191), ((',', 'but'), 1007), (('of', 'the'), 998), (('to', 'be'), 943)]\n",
            "Dataset: test\n",
            "Top 20 unigrams\n",
            "[('.', 4140), ('be', 3348), ('I', 2680), ('the', 1934), (',', 1557), ('to', 1454), ('a', 1389), ('you', 1358), ('it', 1270), ('not', 1186), ('that', 1078), ('!', 1059), ('and', 1036), ('name', 925), (']', 922), ('[', 918), ('do', 881), ('<NUM>', 868), ('of', 818), ('have', 773)]\n",
            "Top 20 bigrams\n",
            "[(('[', 'name'), 894), (('name', ']'), 894), (('do', 'not'), 442), (('it', 'be'), 388), (('I', 'be'), 331), (('.', 'I'), 291), (('be', 'a'), 285), (('be', 'not'), 234), (('!', '!'), 205), (('that', 'be'), 187), (('you', 'be'), 186), (('I', 'do'), 173), ((',', 'I'), 171), (('I', 'have'), 165), (('in', 'the'), 162), (('this', 'be'), 152), (('of', 'the'), 133), (('be', 'the'), 133), ((',', 'but'), 128), (('thank', 'you'), 127)]\n",
            "Dataset: dev\n",
            "Top 20 unigrams\n",
            "[('.', 4160), ('be', 3367), ('I', 2589), ('the', 2072), (',', 1575), ('a', 1429), ('to', 1414), ('you', 1311), ('it', 1293), ('not', 1080), ('that', 1059), ('!', 1049), ('and', 1034), ('name', 906), ('[', 893), (']', 892), ('do', 827), ('<NUM>', 825), ('of', 814), ('this', 774)]\n",
            "Top 20 bigrams\n",
            "[(('[', 'name'), 880), (('name', ']'), 880), (('do', 'not'), 425), (('it', 'be'), 382), (('I', 'be'), 370), (('.', 'I'), 285), (('be', 'a'), 282), (('!', '!'), 203), (('be', 'not'), 197), (('that', 'be'), 183), ((',', 'I'), 173), (('you', 'be'), 173), (('this', 'be'), 171), (('I', 'have'), 155), (('in', 'the'), 153), (('I', 'do'), 141), (('be', 'the'), 139), (('.', 'it'), 123), ((',', 'but'), 118), (('of', 'the'), 116)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rare tokens that appear less than min_freq times, to reduce noise\n",
        "min_freq = 2\n",
        "\n",
        "for data_set in datasets:\n",
        "    dataset_tokens = tokenized_datasets[data_set]  # DataFrame with a 'tokens' column\n",
        "    tokens_series = dataset_tokens['tokens']\n",
        "\n",
        "    token_freq = Counter(token for entry in tokens_series for token in entry)\n",
        "\n",
        "    dataset_tokens['pruned_tokens'] = tokens_series.apply(\n",
        "        lambda tokens: [token for token in tokens if token_freq[token] >= min_freq]\n",
        "    )\n"
      ],
      "metadata": {
        "id": "3d9VfZNdfgMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data_set in datasets:\n",
        "  dataset_tokens = tokenized_datasets[data_set]\n",
        "\n",
        "  tokens_series_normal = dataset_tokens['tokens']\n",
        "  tokens_series_pruned = dataset_tokens['pruned_tokens']\n",
        "\n",
        "  token_freq_normal = Counter(token for entry in tokens_series_normal for token in entry)\n",
        "  token_freq_pruned = Counter(token for entry in tokens_series_pruned for token in entry)\n",
        "\n",
        "  print(f\"Dataset: {data_set}\")\n",
        "  print(f\"Unique tokens before pruning: {len(token_freq_normal)}, Highest freq: {token_freq_normal.most_common(1)}, Lowest freq: {token_freq_normal.most_common()[::-1][:1]}\")\n",
        "  print(f\"Unique tokens after pruning: {len(token_freq_pruned)}, Highest freq: {token_freq_pruned.most_common(1)}, Lowest freq: {token_freq_pruned.most_common()[::-1][:1]}\")\n",
        "  print(f\"Tokens pruned: {len(token_freq_normal) - len(token_freq_pruned)}\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG1-BA0fhFhW",
        "outputId": "46764fc3-d2c4-4dc9-94b7-aa2e9740f2fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: train\n",
            "Unique tokens before pruning: 24493, Highest freq: [('.', 33321)], Lowest freq: [('Huskies', 1)]\n",
            "Unique tokens after pruning: 10777, Highest freq: [('.', 33321)], Lowest freq: [('okie', 2)]\n",
            "Tokens pruned: 13716\n",
            "\n",
            "Dataset: test\n",
            "Unique tokens before pruning: 7207, Highest freq: [('.', 4140)], Lowest freq: [('overdose', 1)]\n",
            "Unique tokens after pruning: 2913, Highest freq: [('.', 4140)], Lowest freq: [('Elmo', 2)]\n",
            "Tokens pruned: 4294\n",
            "\n",
            "Dataset: dev\n",
            "Unique tokens before pruning: 7339, Highest freq: [('.', 4160)], Lowest freq: [('/tiny', 1)]\n",
            "Unique tokens after pruning: 2941, Highest freq: [('.', 4160)], Lowest freq: [('shitbag', 2)]\n",
            "Tokens pruned: 4398\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training: a multi-class Naïve Bayes classifier"
      ],
      "metadata": {
        "id": "HxyEXOpyiUDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdbvUGAVyBEk",
        "outputId": "535871d4-65ad-40c2-b553-3e92f2166839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nb_eval(x, y):\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        x,\n",
        "        y,\n",
        "        test_size = 0.2,\n",
        "        random_state = 42,\n",
        "        stratify = y) # Stratify because the emotions arent evenly distributed, this helps\n",
        "\n",
        "    # ngram_range adds unigrams and bigrams\n",
        "    # min_df, max_df, max_features do thingies\n",
        "\n",
        "    vectorizer = CountVectorizer(\n",
        "        #ngram_range=(1, 2),\n",
        "        min_df=1,\n",
        "        max_df=0.9,\n",
        "        max_features=15000\n",
        "    )\n",
        "\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "    nb = MultinomialNB()\n",
        "    nb.fit(X_train_vec, y_train)\n",
        "\n",
        "    y_pred = nb.predict(X_test_vec)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
        "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "    return nb, vectorizer"
      ],
      "metadata": {
        "id": "cgCse5mc3gY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JdQPm2yJIWjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "df = tokenized_datasets[datasets[0]]\n",
        "\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"Dataset: {datasets[0]}\")\n",
        "print(\"----------------------------------------\")\n",
        "\n",
        "df['normal_tokens_to_str'] = df['tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "df['pruned_tokens_to_str'] = df['pruned_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "print(\"############################################\")\n",
        "print(\"Eval with training data\")\n",
        "print(\"###########################################\")\n",
        "print()\n",
        "\n",
        "nb_normal, vectorizer_normal = nb_eval(df['normal_tokens_to_str'], df[2])\n",
        "print()\n",
        "print(\"################# Pruned ##################\")\n",
        "print()\n",
        "nb_pruned, vectorizer_pruned = nb_eval(df['pruned_tokens_to_str'], df[2])\n",
        "\n",
        "if (nb_normal):\n",
        "  print(nb_normal)\n",
        "\n",
        "if (nb_pruned):\n",
        "  print(nb_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr_DP6ggiYf0",
        "outputId": "6f954289-e0e1-4080-c75a-24ccd3fa34ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Dataset: train\n",
            "----------------------------------------\n",
            "############################################\n",
            "Eval with training data\n",
            "###########################################\n",
            "\n",
            "Accuracy: 0.5723675894324358\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.60      0.21      0.32       859\n",
            "     disgust       0.67      0.02      0.04       100\n",
            "        fear       1.00      0.01      0.02       108\n",
            "         joy       0.62      0.83      0.71      3043\n",
            "     neutral       0.50      0.64      0.56      2564\n",
            "     sadness       0.76      0.12      0.21       465\n",
            "    surprise       0.57      0.13      0.22       772\n",
            "\n",
            "    accuracy                           0.57      7911\n",
            "   macro avg       0.67      0.28      0.30      7911\n",
            "weighted avg       0.59      0.57      0.53      7911\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 184    1    0  225  431    1   17]\n",
            " [  14    2    0   37   45    1    1]\n",
            " [   2    0    1   43   60    0    2]\n",
            " [  15    0    0 2536  480    1   11]\n",
            " [  66    0    0  798 1644   13   43]\n",
            " [   8    0    0  202  193   58    4]\n",
            " [  20    0    0  236  411    2  103]]\n",
            "\n",
            "################# Pruned ##################\n",
            "\n",
            "Accuracy: 0.5809632157755025\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.51      0.28      0.36       859\n",
            "     disgust       0.50      0.05      0.09       100\n",
            "        fear       0.80      0.04      0.07       108\n",
            "         joy       0.65      0.81      0.72      3043\n",
            "     neutral       0.51      0.63      0.56      2564\n",
            "     sadness       0.63      0.23      0.34       465\n",
            "    surprise       0.50      0.22      0.31       772\n",
            "\n",
            "    accuracy                           0.58      7911\n",
            "   macro avg       0.59      0.32      0.35      7911\n",
            "weighted avg       0.58      0.58      0.55      7911\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 239    1    0  182  403    7   27]\n",
            " [  15    5    0   32   42    4    2]\n",
            " [   7    0    4   37   53    2    5]\n",
            " [  41    1    0 2466  492    7   36]\n",
            " [ 121    2    1  710 1603   39   88]\n",
            " [  19    1    0  157  170  107   11]\n",
            " [  31    0    0  197  368    4  172]]\n",
            "MultinomialNB()\n",
            "MultinomialNB()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def helper_evaluation_stats(y_true, y_pred):\n",
        "  # Confusion matrix\n",
        "  print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "\n",
        "  # Classification report (includes precision, recall, f1 for each class)\n",
        "  print(classification_report(y_true, y_pred, digits=3))\n",
        "\n",
        "  # Micro and Macro averages\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
        "  print(f\"Micro Avg - Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
        "\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "  print(f\"Macro Avg - Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
        "\n",
        "  # Average accuracy\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  print(f\"Accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "aheFmSfR7DA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = tokenized_datasets[datasets[1]]\n",
        "df_test['normal_tokens_to_str'] = df_test['tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "df_test['pruned_tokens_to_str'] = df_test['pruned_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "y_true = df_test[2]\n",
        "\n",
        "X_test_normal = vectorizer_normal.transform(df_test['normal_tokens_to_str'])\n",
        "y_pred_normal = nb_normal.predict(X_test_normal)\n",
        "\n",
        "print(\"----------------------------- NORMAL --------------------------------\")\n",
        "helper_evaluation_stats(y_true, y_pred_normal)\n",
        "print()\n",
        "\n",
        "X_test_pruned = vectorizer_pruned.transform(df_test['pruned_tokens_to_str'])\n",
        "y_pred_pruned = nb_pruned.predict(X_test_pruned)\n",
        "\n",
        "print(\"----------------------------- PRUNED --------------------------------\")\n",
        "helper_evaluation_stats(y_true, y_pred_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UhD5ygx6AnN",
        "outputId": "ebc57b89-3235-42b6-d0c7-cb21dd53af31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------- NORMAL --------------------------------\n",
            "Confusion Matrix:\n",
            " [[ 111    0    0  151  302    2    6]\n",
            " [   9    2    0   34   30    0    1]\n",
            " [   1    0    0   36   40    1    2]\n",
            " [  13    0    0 1541  293    0   16]\n",
            " [  51    0    0  523 1005    6   21]\n",
            " [  12    0    0  137   99   30    5]\n",
            " [   8    0    0  154  252    1   73]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger      0.541     0.194     0.286       572\n",
            "     disgust      1.000     0.026     0.051        76\n",
            "        fear      0.000     0.000     0.000        80\n",
            "         joy      0.598     0.827     0.694      1863\n",
            "     neutral      0.497     0.626     0.554      1606\n",
            "     sadness      0.750     0.106     0.186       283\n",
            "    surprise      0.589     0.150     0.239       488\n",
            "\n",
            "    accuracy                          0.556      4968\n",
            "   macro avg      0.568     0.276     0.287      4968\n",
            "weighted avg      0.563     0.556     0.507      4968\n",
            "\n",
            "Micro Avg - Precision: 0.556, Recall: 0.556, F1: 0.556\n",
            "Macro Avg - Precision: 0.568, Recall: 0.276, F1: 0.287\n",
            "Accuracy: 0.556\n",
            "\n",
            "----------------------------- PRUNED --------------------------------\n",
            "Confusion Matrix:\n",
            " [[ 125    0    0  144  284    5   14]\n",
            " [  11    4    0   31   27    0    3]\n",
            " [   3    0    2   32   39    3    1]\n",
            " [  21    0    0 1524  300    1   17]\n",
            " [  50    0    0  504 1016    5   31]\n",
            " [  11    0    0  109  104   52    7]\n",
            " [   9    0    0  137  244    2   96]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger      0.543     0.219     0.312       572\n",
            "     disgust      1.000     0.053     0.100        76\n",
            "        fear      1.000     0.025     0.049        80\n",
            "         joy      0.614     0.818     0.702      1863\n",
            "     neutral      0.504     0.633     0.561      1606\n",
            "     sadness      0.765     0.184     0.296       283\n",
            "    surprise      0.568     0.197     0.292       488\n",
            "\n",
            "    accuracy                          0.567      4968\n",
            "   macro avg      0.714     0.304     0.330      4968\n",
            "weighted avg      0.587     0.567     0.528      4968\n",
            "\n",
            "Micro Avg - Precision: 0.567, Recall: 0.567, F1: 0.567\n",
            "Macro Avg - Precision: 0.714, Recall: 0.304, F1: 0.330\n",
            "Accuracy: 0.567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RESULTS\n",
        "Regular accuracy:\n",
        "\n",
        "\n",
        "*   Pruned: 56.7%\n",
        "*   Not Pruned: 55.6%\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "With Bigrams added\n",
        "\n",
        "\n",
        "*   Pruned: 58.5%\n",
        "*   Not Pruned: 58.7%\n",
        "\n",
        "\n",
        "\n",
        "The Achieved results are lower than the reference research paper, and as can be seen pruning the dataset reduces accuracy. Which means that useful tokens might be removed."
      ],
      "metadata": {
        "id": "HIUTDlR6yIfW"
      }
    }
  ]
}